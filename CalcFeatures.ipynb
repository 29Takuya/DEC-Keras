{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Keras implementation of deep embedder to improve clustering, inspired by:\n",
    "\"Unsupervised Deep Embedding for Clustering Analysis\" (Xie et al, ICML 2016)\n",
    "\n",
    "Definition can accept somewhat custom neural networks. Defaults are from paper.\n",
    "'''\n",
    "import sys\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.preprocessing import normalize\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "if (sys.version[0] == 2):\n",
    "    import cPickle as pickle\n",
    "else:\n",
    "    import pickle\n",
    "import numpy as np\n",
    "\n",
    "import IPython\n",
    "\n",
    "class ClusteringLayer(Layer):\n",
    "    '''\n",
    "    Clustering layer which converts latent space Z of input layer\n",
    "    into a probability vector for each cluster defined by its centre in\n",
    "    Z-space. Use Kullback-Leibler divergence as loss, with a probability\n",
    "    target distribution.\n",
    "    # Arguments\n",
    "        output_dim: int > 0. Should be same as number of clusters.\n",
    "        input_dim: dimensionality of the input (integer).\n",
    "            This argument (or alternatively, the keyword argument `input_shape`)\n",
    "            is required when using this layer as the first layer in a model.\n",
    "        weights: list of Numpy arrays to set as initial weights.\n",
    "            The list should have 2 elements, of shape `(input_dim, output_dim)`\n",
    "            and (output_dim,) for weights and biases respectively.\n",
    "        alpha: parameter in Student's t-distribution. Default is 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(nb_samples, input_dim)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(nb_samples, output_dim)`.\n",
    "    '''\n",
    "    def __init__(self, output_dim, input_dim=None, weights=None, alpha=1.0, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.alpha = alpha\n",
    "        # kmeans cluster centre locations\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = [InputSpec(ndim=2)]\n",
    "\n",
    "        if self.input_dim:\n",
    "            kwargs['input_shape'] = (self.input_dim,)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = [InputSpec(dtype=K.floatx(),\n",
    "                                     shape=(None, input_dim))]\n",
    "        if self.initial_weights == None:\n",
    "            initial_weight = np.loadtxt()\n",
    "        else:\n",
    "            self.W = K.variable(self.initial_weights)\n",
    "            self.trainable_weights = [self.W]\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        q = 1.0/(1.0 + K.sqrt(K.sum(K.square(K.expand_dims(x, 1) - self.W), axis=2))**2 /self.alpha)\n",
    "        q = q**((self.alpha+1.0)/2.0)\n",
    "        q = K.transpose(K.transpose(q)/K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'output_dim': self.output_dim,\n",
    "                  'input_dim': self.input_dim}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class DeepEmbeddingClustering(object):\n",
    "    def __init__(self,\n",
    "                 n_clusters,\n",
    "                 input_dim,\n",
    "                 encoded=None,\n",
    "                 decoded=None,\n",
    "                 alpha=1.0,\n",
    "                 pretrained_weights=None,\n",
    "                 cluster_centres=None,\n",
    "                 batch_size=256,\n",
    "                 **kwargs):\n",
    "\n",
    "        super(DeepEmbeddingClustering, self).__init__()\n",
    "\n",
    "        self.n_clusters = n_clusters\n",
    "        self.input_dim = input_dim\n",
    "        self.encoded = encoded\n",
    "        self.decoded = decoded\n",
    "        self.alpha = alpha\n",
    "        self.pretrained_weights = pretrained_weights\n",
    "        self.cluster_centres = cluster_centres\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.learning_rate = 0.1\n",
    "        self.iters_lr_update = 20000\n",
    "        self.lr_change_rate = 0.1\n",
    "\n",
    "        # greedy layer-wise training before end-to-end training:\n",
    "\n",
    "        self.encoders_dims = [self.input_dim, 500, 500, 2000, 10]\n",
    "\n",
    "        self.input_layer = Input(shape=(self.input_dim,), name='input')\n",
    "        dropout_fraction = 0.2\n",
    "        init_stddev = 0.01\n",
    "\n",
    "        self.layer_wise_autoencoders = []\n",
    "        self.encoders = []\n",
    "        self.decoders = []\n",
    "        for i  in range(1, len(self.encoders_dims)):\n",
    "            \n",
    "            encoder_activation = 'linear' if i == (len(self.encoders_dims) - 1) else 'relu'\n",
    "            encoder = Dense(self.encoders_dims[i], activation=encoder_activation,\n",
    "                            input_shape=(self.encoders_dims[i-1],),\n",
    "                            kernel_initializer=RandomNormal(mean=0.0, stddev=init_stddev, seed=None),\n",
    "                            bias_initializer='zeros', name='encoder_dense_%d'%i)\n",
    "            self.encoders.append(encoder)\n",
    "\n",
    "            decoder_index = len(self.encoders_dims) - i\n",
    "            decoder_activation = 'linear' if i == 1 else 'relu'\n",
    "            decoder = Dense(self.encoders_dims[i-1], activation=decoder_activation,\n",
    "                            kernel_initializer=RandomNormal(mean=0.0, stddev=init_stddev, seed=None),\n",
    "                            bias_initializer='zeros',\n",
    "                            name='decoder_dense_%d'%decoder_index)\n",
    "            self.decoders.append(decoder)\n",
    "\n",
    "            autoencoder = Sequential([\n",
    "                Dropout(dropout_fraction, input_shape=(self.encoders_dims[i-1],), \n",
    "                        name='encoder_dropout_%d'%i),\n",
    "                encoder,\n",
    "                Dropout(dropout_fraction, name='decoder_dropout_%d'%decoder_index),\n",
    "                decoder\n",
    "            ])\n",
    "            autoencoder.compile(loss='mse', optimizer=SGD(lr=self.learning_rate, decay=0, momentum=0.9))\n",
    "            self.layer_wise_autoencoders.append(autoencoder)\n",
    "\n",
    "        # build the end-to-end autoencoder for finetuning\n",
    "        # Note that at this point dropout is discarded\n",
    "        self.encoder = Sequential(self.encoders)\n",
    "        self.encoder.compile(loss='mse', optimizer=SGD(lr=self.learning_rate, decay=0, momentum=0.9))\n",
    "        self.decoders.reverse()\n",
    "        self.autoencoder = Sequential(self.encoders + self.decoders)\n",
    "        self.autoencoder.compile(loss='mse', optimizer=SGD(lr=self.learning_rate, decay=0, momentum=0.9))\n",
    "\n",
    "        if cluster_centres is not None:\n",
    "            assert cluster_centres.shape[0] == self.n_clusters\n",
    "            assert cluster_centres.shape[1] == self.encoder.layers[-1].output_dim\n",
    "\n",
    "        if self.pretrained_weights is not None:\n",
    "            self.autoencoder.load_weights(self.pretrained_weights)\n",
    "\n",
    "    def p_mat(self, q):\n",
    "        weight = q**2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "\n",
    "    def initialize(self, X, save_autoencoder=False, layerwise_pretrain_iters=50000, finetune_iters=100000):\n",
    "        if self.pretrained_weights is None:\n",
    "\n",
    "            iters_per_epoch = int(len(X) / self.batch_size)\n",
    "            layerwise_epochs = max(int(layerwise_pretrain_iters / iters_per_epoch), 1)\n",
    "            finetune_epochs = max(int(finetune_iters / iters_per_epoch), 1)\n",
    "\n",
    "            print('layerwise pretrain')\n",
    "            current_input = X\n",
    "            lr_epoch_update = max(1, self.iters_lr_update / float(iters_per_epoch))\n",
    "            \n",
    "            def step_decay(epoch):\n",
    "                initial_rate = self.learning_rate\n",
    "                factor = int(epoch / lr_epoch_update)\n",
    "                lr = initial_rate / (10 ** factor)\n",
    "                return lr\n",
    "            lr_schedule = LearningRateScheduler(step_decay)\n",
    "\n",
    "            for i, autoencoder in enumerate(self.layer_wise_autoencoders):\n",
    "                if i > 0:\n",
    "                    weights = self.encoders[i-1].get_weights()\n",
    "                    dense_layer = Dense(self.encoders_dims[i], input_shape=(current_input.shape[1],),\n",
    "                                        activation='relu', weights=weights,\n",
    "                                        name='encoder_dense_copy_%d'%i)\n",
    "                    encoder_model = Sequential([dense_layer])\n",
    "                    encoder_model.compile(loss='mse', optimizer=SGD(lr=self.learning_rate, decay=0, momentum=0.9))\n",
    "                    current_input = encoder_model.predict(current_input)\n",
    "\n",
    "                autoencoder.fit(current_input, current_input, \n",
    "                                batch_size=self.batch_size, epochs=layerwise_epochs, callbacks=[lr_schedule])\n",
    "                self.autoencoder.layers[i].set_weights(autoencoder.layers[1].get_weights())\n",
    "                self.autoencoder.layers[len(self.autoencoder.layers) - i - 1].set_weights(autoencoder.layers[-1].get_weights())\n",
    "            \n",
    "            print('Finetuning autoencoder')\n",
    "            \n",
    "            #update encoder and decoder weights:\n",
    "            self.autoencoder.fit(X, X, batch_size=self.batch_size, epochs=finetune_epochs, callbacks=[lr_schedule])\n",
    "\n",
    "            if save_autoencoder:\n",
    "                self.autoencoder.save_weights('autoencoder.h5')\n",
    "        else:\n",
    "            print('Loading pretrained weights for autoencoder.')\n",
    "            self.autoencoder.load_weights(self.pretrained_weights)\n",
    "\n",
    "        # update encoder, decoder\n",
    "        # TODO: is this needed? Might be redundant...\n",
    "        for i in range(len(self.encoder.layers)):\n",
    "            self.encoder.layers[i].set_weights(self.autoencoder.layers[i].get_weights())\n",
    "\n",
    "        # initialize cluster centres using k-means\n",
    "        print('Initializing cluster centres with k-means.')\n",
    "        if self.cluster_centres is None:\n",
    "            #IPython.embed()\n",
    "            m=len(X)\n",
    "            X_tmp = X[np.random.choice(range(m), 10000),:]\n",
    "            kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n",
    "            kmeans.fit(self.encoder.predict(X_tmp))\n",
    "            self.y_pred = kmeans.predict(self.encoder.predict(X))\n",
    "            self.cluster_centres = kmeans.cluster_centers_\n",
    "\n",
    "        # prepare DEC model\n",
    "        #self.DEC = Model(inputs=self.input_layer,\n",
    "        #                 outputs=ClusteringLayer(self.n_clusters,\n",
    "        #                                        weights=self.cluster_centres,\n",
    "        #                                        name='clustering')(self.encoder))\n",
    "        self.DEC = Sequential([self.encoder,\n",
    "                             ClusteringLayer(self.n_clusters,\n",
    "                                                weights=self.cluster_centres,\n",
    "                                                name='clustering')])\n",
    "        self.DEC.compile(loss='kullback_leibler_divergence', optimizer='adadelta')\n",
    "        return\n",
    "\n",
    "    def cluster_acc(self, y_true, y_pred):\n",
    "        assert y_pred.size == y_true.size\n",
    "        D = max(y_pred.max(), y_true.max())+1\n",
    "        w = np.zeros((D, D), dtype=np.int64)\n",
    "        for i in range(y_pred.size):\n",
    "            w[y_pred[i], y_true[i]] += 1\n",
    "        ind = linear_assignment(w.max() - w)\n",
    "        return sum([w[i, j] for i, j in ind])*1.0/y_pred.size, w\n",
    "\n",
    "    def cluster(self, X, y=None,\n",
    "                tol=0.01, update_interval=None,\n",
    "                iter_max=1e7,\n",
    "                save_interval=None,\n",
    "                **kwargs):\n",
    "\n",
    "        if update_interval is None:\n",
    "            # 1 epochs\n",
    "            update_interval = int(X.shape[0]/self.batch_size)\n",
    "        print('Update interval', update_interval)\n",
    "\n",
    "        if save_interval is None:\n",
    "            # 50 epochs\n",
    "            #save_interval = int(X.shape[0]/self.batch_size*50)\n",
    "            save_interval = update_interval*10\n",
    "        print('Save interval', save_interval)\n",
    "\n",
    "        assert save_interval >= update_interval\n",
    "\n",
    "        train = True\n",
    "        iteration, index = 0, 0\n",
    "        self.accuracy = []\n",
    "\n",
    "        while train:\n",
    "            sys.stdout.write('\\r')\n",
    "            # cutoff iteration\n",
    "            if iter_max < iteration:\n",
    "                print('Reached maximum iteration limit. Stopping training.')\n",
    "                return self.y_pred\n",
    "\n",
    "            # update (or initialize) probability distributions and propagate weight changes\n",
    "            # from DEC model to encoder.\n",
    "            if iteration % update_interval == 0:\n",
    "                self.q = self.DEC.predict(X, verbose=0)\n",
    "                self.p = self.p_mat(self.q)\n",
    "\n",
    "                y_pred = self.q.argmax(1)\n",
    "                #IPython.embed()\n",
    "                delta_label = ((y_pred != self.y_pred).sum().astype(np.float32) / y_pred.shape[0])\n",
    "                if y is not None:\n",
    "                    acc = self.cluster_acc(y, y_pred)[0]\n",
    "                    self.accuracy.append(acc)\n",
    "                    print('Iteration '+str(iteration)+', Accuracy '+str(np.round(acc, 5)))\n",
    "                else:\n",
    "                    print(str(np.round(delta_label*100, 5))+'% change in label assignment')\n",
    "\n",
    "                if delta_label < tol:\n",
    "                    print('Reached tolerance threshold. Stopping training.')\n",
    "                    train = False\n",
    "                    continue\n",
    "                else:\n",
    "                    self.y_pred = y_pred\n",
    "\n",
    "                for i in range(len(self.encoder.layers)):\n",
    "                    self.encoder.layers[i].set_weights(self.DEC.layers[0].layers[i].get_weights())\n",
    "                self.cluster_centres = self.DEC.layers[-1].get_weights()[0]\n",
    "\n",
    "            # train on batch\n",
    "            sys.stdout.write('Iteration %d, ' % iteration)\n",
    "            if (index+1)*self.batch_size > X.shape[0]:\n",
    "                loss = self.DEC.train_on_batch(X[index*self.batch_size::], self.p[index*self.batch_size::])\n",
    "                index = 0\n",
    "                sys.stdout.write('Loss %f' % loss)\n",
    "            else:\n",
    "                loss = self.DEC.train_on_batch(X[index*self.batch_size:(index+1) * self.batch_size],\n",
    "                                               self.p[index*self.batch_size:(index+1) * self.batch_size])\n",
    "                sys.stdout.write('Loss %f' % loss)\n",
    "                index += 1\n",
    "\n",
    "            # save intermediate\n",
    "            if iteration % save_interval == 0:\n",
    "                z = self.encoder.predict(X)\n",
    "                m=len(z)\n",
    "                z_tmp = z[np.random.choice(range(m), 10000),:]\n",
    "                #IPython.embed()\n",
    "                pca = PCA(n_components=2).fit(z_tmp)\n",
    "                z_2d = pca.transform(z)\n",
    "                clust_2d = pca.transform(self.cluster_centres)\n",
    "                # save states for visualization\n",
    "                pickle.dump({'z_2d': z_2d, 'clust_2d': clust_2d, 'q': self.q, 'p': self.p},\n",
    "                            open('c'+str(iteration)+'.pkl', 'wb'))\n",
    "                # save DEC model checkpoints\n",
    "                self.DEC.save('DEC_model_'+str(iteration)+'.h5')\n",
    "\n",
    "            iteration += 1\n",
    "            sys.stdout.flush()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = DeepEmbeddingClustering(n_clusters=100, input_dim=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initital_weights None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "initial_value must be specified.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-e52a2f839d33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DEC_model_0.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'ClusteringLayer'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mClusteringLayer\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config file.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;31m# set weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0;34m'Maybe you meant to use '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                         '`Sequential.from_config(config)`?')\n\u001b[0;32m--> 307\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     52\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    137\u001b[0m                 return cls.from_config(config['config'],\n\u001b[1;32m    138\u001b[0m                                        custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[0;32m--> 139\u001b[0;31m                                                            list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m   1208\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1210\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1211\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    467\u001b[0m                           output_shapes=[self.outputs[0]._keras_shape])\n\u001b[1;32m    468\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[1;32m    568\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-7111defb9642>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     64\u001b[0m                                      shape=(None, input_dim))]\n\u001b[1;32m     65\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'initital_weights'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mvariable\u001b[0;34m(value, dtype, name)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_learning_phase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_convert_string_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope)\u001b[0m\n\u001b[1;32m    198\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m           \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m           expected_shape=expected_shape)\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, expected_shape)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpected_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"initial_value must be specified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0minit_from_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: initial_value must be specified."
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('DEC_model_0.h5', custom_objects={'ClusteringLayer':ClusteringLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = DeepEmbeddingClustering(n_clusters=100, input_dim=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = ClusteringLayer(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_dim': None,\n",
       " 'name': 'clustering_layer_1',\n",
       " 'output_dim': 100,\n",
       " 'trainable': True}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
